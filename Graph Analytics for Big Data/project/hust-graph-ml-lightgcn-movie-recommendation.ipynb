{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-20T20:35:23.022758Z","iopub.execute_input":"2024-10-20T20:35:23.023149Z","iopub.status.idle":"2024-10-20T20:35:23.029331Z","shell.execute_reply.started":"2024-10-20T20:35:23.023113Z","shell.execute_reply":"2024-10-20T20:35:23.028431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n!pip install tensorly\n!pip install torch-geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T20:35:23.030770Z","iopub.execute_input":"2024-10-20T20:35:23.031075Z","iopub.status.idle":"2024-10-20T20:36:03.029955Z","shell.execute_reply.started":"2024-10-20T20:35:23.031044Z","shell.execute_reply":"2024-10-20T20:36:03.028785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import collections\nimport math\nimport os\nimport os.path as osp\nfrom tqdm import tqdm\nfrom typing import List\nimport random\nimport time\nimport zipfile\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\npd.options.display.max_rows = 10\nfrom sklearn import metrics\nfrom tensorly import decomposition\n\nimport torch\nfrom torch.functional import tensordot\nfrom torch import nn, optim, Tensor\nimport torch_geometric\nfrom torch_geometric.data import Dataset, Data, download_url, extract_zip\nfrom torch_geometric.nn import MessagePassing\nfrom torch_geometric.typing import Adj","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T20:36:03.031395Z","iopub.execute_input":"2024-10-20T20:36:03.031717Z","iopub.status.idle":"2024-10-20T20:36:08.981038Z","shell.execute_reply.started":"2024-10-20T20:36:03.031682Z","shell.execute_reply":"2024-10-20T20:36:08.980084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"PyTorch has version {torch.__version__}\")\nprint(f\"Torch version: {torch.__version__}\")\nprint(f\"Cuda available: {torch.cuda.is_available()}\")\nprint(f\"Torch geometric version: {torch_geometric.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T20:36:08.983552Z","iopub.execute_input":"2024-10-20T20:36:08.984746Z","iopub.status.idle":"2024-10-20T20:36:08.990152Z","shell.execute_reply.started":"2024-10-20T20:36:08.984698Z","shell.execute_reply":"2024-10-20T20:36:08.989309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rating_threshold = 3 \n\nconfig_dict = {\n    \"num_samples_per_user\": 500,\n    \"num_users\": 200,\n\n    \"epochs\": 100,\n    \"batch_size\": 128,\n    \"lr\": 0.001,\n    \"weight_decay\": 0.1,\n\n    \"embedding_size\": 64,\n    \"num_layers\": 5,\n    \"K\": 10,\n    \"mf_rank\": 8,\n\n    \"minibatch_per_print\": 100,\n     \"epochs_per_print\": 1,\n\n    \"val_frac\": 0.2,\n    \"test_frac\": 0.1,\n\n    \"model_name\": \"model.pth\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T20:36:08.991677Z","iopub.execute_input":"2024-10-20T20:36:08.992004Z","iopub.status.idle":"2024-10-20T20:36:09.017276Z","shell.execute_reply.started":"2024-10-20T20:36:08.991964Z","shell.execute_reply":"2024-10-20T20:36:09.016176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_PATH = \"https://files.grouplens.org/datasets/movielens/ml-1m.zip\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T20:36:09.018456Z","iopub.execute_input":"2024-10-20T20:36:09.018813Z","iopub.status.idle":"2024-10-20T20:36:09.087982Z","shell.execute_reply.started":"2024-10-20T20:36:09.018771Z","shell.execute_reply":"2024-10-20T20:36:09.086912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def trans_ml(dat, thres):\n    \"\"\"\n    Transform function that assign non-negative entries >= thres 1, and non-\n    negative entries <= thres 0. Keep other entries the same.\n    \"\"\"\n    thres = thres[0]\n    matrix = dat['edge_index']\n    matrix[(matrix < thres) & (matrix > -1)] = 0\n    matrix[(matrix >= thres)] = 1\n    dat['edge_index'] = matrix\n    return dat\n\n\nclass MovieLens(Dataset):\n    def __init__(self, root, transform=None, pre_transform=None,\n            transform_args=None, pre_transform_args=None):\n        \"\"\"\n        root = where the dataset should be stored. This folder is split\n        into raw_dir (downloaded dataset) and processed_dir (process data).\n        \"\"\"\n        super(MovieLens, self).__init__(root, transform, pre_transform)\n        self.transform = transform\n        self.pre_transform = pre_transform\n        self.transform_args = transform_args\n        self.pre_transform_args = pre_transform_args\n\n    @property\n    def raw_file_names(self):\n        return \"ml-1m.zip\"\n\n    @property\n    def processed_file_names(self):\n        return [\"data_movielens.pt\"]\n\n    def download(self):\n        # Download to `self.raw_dir`.\n        download_url(DATA_PATH, self.raw_dir)\n\n    def _load(self):\n        print(self.raw_dir)\n        # extract_zip(self.raw_paths[0], self.raw_dir)\n        with zipfile.ZipFile(self.raw_paths[0], 'r') as zip_ref:\n            zip_ref.extractall(self.raw_dir)\n        unames = ['user_id', 'gender', 'age', 'occupation', 'zip']\n        users = pd.read_table(self.raw_dir+'/ml-1m/users.dat',\n                              sep='::', header=None, names=unames,\n                              engine='python', encoding='latin-1')\n        rnames = ['user_id', 'movie_id', 'rating', 'timestamp']\n        ratings = pd.read_table(self.raw_dir+'/ml-1m/ratings.dat', sep='::',\n                                header=None, names=rnames, engine='python',\n                                encoding='latin-1')\n        mnames = ['movie_id', 'title', 'genres']\n        movies = pd.read_table(self.raw_dir+'/ml-1m/movies.dat', sep='::',\n                               header=None, names=mnames, engine='python',\n                               encoding='latin-1')\n        dat = pd.merge(pd.merge(ratings, users), movies)\n\n        return users, ratings, movies, dat\n\n    def process(self):\n        print('run process')\n        # load information from file\n        users, ratings, movies, dat = self._load()\n\n        users = users['user_id']\n        movies = movies['movie_id']\n\n        num_users = config_dict[\"num_users\"]\n        if num_users != -1:\n            users = users[:num_users]\n\n        user_ids = range(len(users))\n        movie_ids = range(len(movies))\n\n        user_to_id = dict(zip(users, user_ids))\n        movie_to_id = dict(zip(movies, movie_ids))\n\n        # get adjacency info\n        self.num_user = users.shape[0]\n        self.num_item = movies.shape[0]\n\n        # initialize the adjacency matrix\n        rat = torch.zeros(self.num_user, self.num_item)\n\n        for index, row in ratings.iterrows():\n            user, movie, rating = row[:3]\n            if num_users != -1:\n                if user not in user_to_id: break\n            # create ratings matrix where (i, j) entry represents the ratings\n            # of movie j given by user i.\n            rat[user_to_id[user], movie_to_id[movie]] = rating\n\n        # create Data object\n        data = Data(edge_index = rat,\n                    raw_edge_index = rat.clone(),\n                    data = ratings,\n                    users = users,\n                    items = movies)\n\n        # apply any pre-transformation\n        if self.pre_transform is not None:\n            data = self.pre_transform(data, self.pre_transform_args)\n\n        # apply any post_transformation\n        # if self.transform is not None:\n        #     # data = self.transform(data, self.transform_args)\n        data = self.transform(data, [rating_threshold])\n\n        # save the processed data into .pt file\n        torch.save(data, osp.join(self.processed_dir, f'data_movielens.pt'))\n        print('process finished')\n\n    def len(self):\n        \"\"\"\n        return the number of examples in your graph\n        \"\"\"\n        # TODO: how to define number of examples\n        return\n\n    def get(self):\n        \"\"\"\n        The logic to load a single graph\n        \"\"\"\n        data = torch.load(osp.join(self.processed_dir, 'data_movielens.pt'))\n        return data\n\n    def train_val_test_split(self, val_frac=0.2, test_frac=0.1):\n        \"\"\"\n        Return two mask matrices (M, N) that represents edges present in the\n        train and validation set\n        \"\"\"\n        try:\n            self.num_user, self.num_item\n        except AttributeError:\n            data = self.get()\n            self.num_user = len(data[\"users\"].unique())\n            self.num_item = len(data[\"items\"].unique())\n        # get number of edges masked for training and validation\n        num_train_replaced = \\\n            round((test_frac+val_frac)*self.num_user*self.num_item)\n        num_val_show = round(val_frac*self.num_user*self.num_item)\n\n        # edges masked during training\n        indices_user = np.random.randint(0, self.num_user, num_train_replaced)\n        indices_item = np.random.randint(0, self.num_item, num_train_replaced)\n\n        # sample part of edges from training stage to be unmasked during\n        # validation\n        indices_val_user = np.random.choice(indices_user, num_val_show)\n        indices_val_item = np.random.choice(indices_item, num_val_show)\n\n        train_mask = torch.ones(self.num_user, self.num_item)\n        train_mask[indices_user, indices_item] = 0\n\n        val_mask = train_mask.clone()\n        val_mask[indices_val_user, indices_val_item] = 1\n\n        test_mask = torch.ones_like(train_mask)\n\n        return train_mask, val_mask, test_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T20:36:09.089512Z","iopub.execute_input":"2024-10-20T20:36:09.089858Z","iopub.status.idle":"2024-10-20T20:36:09.175465Z","shell.execute_reply.started":"2024-10-20T20:36:09.089818Z","shell.execute_reply":"2024-10-20T20:36:09.173999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LightGCNConv(MessagePassing):\n    r\"\"\"The neighbor aggregation operator from the `\"LightGCN: Simplifying and\n    Powering Graph Convolution Network for Recommendation\"\n    <https://arxiv.org/abs/2002.02126#>`_ paper\n\n    Args:\n        in_channels (int): Size of each input sample, or :obj:`-1` to derive\n            the size from the first input(s) to the forward method.\n        out_channels (int): Size of each output sample.\n        num_users (int): Number of users for recommendation.\n        num_items (int): Number of items to recommend.\n        **kwargs (optional): Additional arguments of\n            :class:`torch_geometric.nn.conv.MessagePassing`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int,\n                 num_users: int, num_items: int, **kwargs):\n        super(LightGCNConv, self).__init__(**kwargs)\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        self.num_users = num_users\n        self.num_items = num_items\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        pass  # There are no layer parameters to learn.\n\n    def forward(self, x: Tensor, edge_index: Adj) -> Tensor:\n        \"\"\"Performs neighborhood aggregation for user/item embeddings.\"\"\"\n        user_item = \\\n                torch.zeros(self.num_users, self.num_items, device=x.device)\n        user_item[edge_index[:, 0], edge_index[:, 1]] = 1\n        user_neighbor_counts = torch.sum(user_item, axis=1)\n        item_neightbor_counts = torch.sum(user_item, axis=0)\n        # Compute weight for aggregation: 1 / sqrt(N_u * N_i)\n        weights = user_item / torch.sqrt(\n                user_neighbor_counts.repeat(self.num_items, 1).T \\\n                * item_neightbor_counts.repeat(self.num_users, 1))\n        weights = torch.nan_to_num(weights, nan=0)\n        out = torch.concat((weights.T @ x[:self.num_users],\n                            weights @ x[self.num_users:]), 0)\n        return out\n\n    def __repr__(self):\n        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n                                   self.out_channels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T20:36:09.177272Z","iopub.execute_input":"2024-10-20T20:36:09.177599Z","iopub.status.idle":"2024-10-20T20:36:09.258382Z","shell.execute_reply.started":"2024-10-20T20:36:09.177566Z","shell.execute_reply":"2024-10-20T20:36:09.257273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LightGCN(nn.Module):\n    def __init__(self,\n                 config: dict,\n                 device=None,\n                 **kwargs):\n        super().__init__()\n\n        self.num_users  = config[\"n_users\"]\n        self.num_items  = config[\"m_items\"]\n        self.embedding_size = config[\"embedding_size\"]\n        self.in_channels = self.embedding_size\n        self.out_channels = self.embedding_size\n        self.num_layers = config[\"num_layers\"]\n\n        # 0-th layer embedding.\n        self.embedding_user_item = torch.nn.Embedding(\n            num_embeddings=self.num_users + self.num_items,\n            embedding_dim=self.embedding_size)\n        self.alpha = None\n\n        # random normal init seems to be a better choice when lightGCN actually\n        # don't use any non-linear activation function\n        nn.init.normal_(self.embedding_user_item.weight, std=0.1)\n        print('use NORMAL distribution initilizer')\n\n        self.f = nn.Sigmoid()\n\n        self.convs = nn.ModuleList()\n        self.convs.append(LightGCNConv(\n                self.embedding_size, self.embedding_size,\n                num_users=self.num_users, num_items=self.num_items, **kwargs))\n\n        for _ in range(1, self.num_layers):\n            self.convs.append(\n                LightGCNConv(\n                        self.embedding_size, self.embedding_size,\n                        num_users=self.num_users, num_items=self.num_items,\n                        **kwargs))\n\n        self.device = None\n        if device is not None:\n            self.convs.to(device)\n            self.device = device\n\n    def reset_parameters(self):\n        for conv in self.convs:\n            conv.reset_parameters()\n\n    def forward(self, x: Tensor, edge_index: Adj, *args, **kwargs) -> Tensor:\n        xs: List[Tensor] = []\n\n        edge_index = torch.nonzero(edge_index)\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index, *args, **kwargs)\n            if self.device is not None:\n                x = x.to(self.device)\n            xs.append(x)\n        xs = torch.stack(xs)\n\n        self.alpha = 1 / (1 + self.num_layers) * torch.ones(xs.shape)\n        if self.device is not None:\n            self.alpha = self.alpha.to(self.device)\n            xs = xs.to(self.device)\n        x = (xs * self.alpha).sum(dim=0)  # Sum along K layers.\n        return x\n\n    def __repr__(self) -> str:\n        return (f'{self.__class__.__name__}({self.in_channels}, '\n                f'{self.out_channels}, num_layers={self.num_layers})')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T20:36:09.262380Z","iopub.execute_input":"2024-10-20T20:36:09.262777Z","iopub.status.idle":"2024-10-20T20:36:09.350048Z","shell.execute_reply.started":"2024-10-20T20:36:09.262740Z","shell.execute_reply":"2024-10-20T20:36:09.349006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def getUsersRating(model, users, data):\n    \"\"\" Get the embedding of users\n    INPUT:\n        model: the LightGCN model you are training on\n        users: this is the user index (note: use 0-indexed and not user number,\n            which is 1-indexed)\n        data: the entire data, used to fetch all users and all items\n    \"\"\"\n    all_users_items = model(model.embedding_user_item.weight.clone(),\n                            data[\"edge_index\"])\n    all_users = all_users_items[:len(data[\"users\"])]\n    items_emb = all_users_items[len(data[\"users\"]):]\n    users_emb = all_users[users.long()]\n    rating = model.f(torch.matmul(users_emb, items_emb.t()))\n    return rating\n\ndef getEmbedding(model, users, pos, neg, data, mask):\n    \"\"\"\n    INPUT:\n        model: the LightGCN model you are training on\n        users: this is the user index (note: use 0-indexed and not user number,\n            which is 1-indexed)\n        pos: positive index corresponding to an item that the user like\n        neg: negative index corresponding to an item that the user doesn't like\n        data: the entire data, used to fetch all users and all items\n        mask: Masking matrix indicating edges present in the current\n            train / validation / test set.\n    \"\"\"\n    # assuming we always search for users and items by their indices (instead of\n    # user/item number)\n    all_users_items = model(model.embedding_user_item.weight.clone(),\n                            data[\"edge_index\"] * mask)\n    all_users = all_users_items[:len(data[\"users\"])]\n    all_items = all_users_items[len(data[\"users\"]):]\n    users_emb = all_users[users]\n    pos_emb = all_items[pos]\n    neg_emb = all_items[neg]\n    n_user = len(data[\"users\"])\n    users_emb_ego = model.embedding_user_item(users)\n    # offset the index to fetch embedding from user_item\n    pos_emb_ego = model.embedding_user_item(pos + n_user)\n    neg_emb_ego = model.embedding_user_item(neg + n_user)\n    return users_emb, pos_emb, neg_emb, users_emb_ego, pos_emb_ego, neg_emb_ego","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T20:36:09.351687Z","iopub.execute_input":"2024-10-20T20:36:09.352102Z","iopub.status.idle":"2024-10-20T20:36:09.426822Z","shell.execute_reply.started":"2024-10-20T20:36:09.352058Z","shell.execute_reply":"2024-10-20T20:36:09.425878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def getUsersRating(model, users, data):\n    \"\"\" Get the embedding of users\n    INPUT:\n        model: the LightGCN model you are training on\n        users: this is the user index (note: use 0-indexed and not user number,\n            which is 1-indexed)\n        data: the entire data, used to fetch all users and all items\n    \"\"\"\n    all_users_items = model(model.embedding_user_item.weight.clone(),\n                            data[\"edge_index\"])\n    all_users = all_users_items[:len(data[\"users\"])]\n    items_emb = all_users_items[len(data[\"users\"]):]\n    users_emb = all_users[users.long()]\n    rating = model.f(torch.matmul(users_emb, items_emb.t()))\n    return rating\n\ndef getEmbedding(model, users, pos, neg, data, mask):\n    \"\"\"\n    INPUT:\n        model: the LightGCN model you are training on\n        users: this is the user index (note: use 0-indexed and not user number,\n            which is 1-indexed)\n        pos: positive index corresponding to an item that the user like\n        neg: negative index corresponding to an item that the user doesn't like\n        data: the entire data, used to fetch all users and all items\n        mask: Masking matrix indicating edges present in the current\n            train / validation / test set.\n    \"\"\"\n    # assuming we always search for users and items by their indices (instead of\n    # user/item number)\n    all_users_items = model(model.embedding_user_item.weight.clone(),\n                            data[\"edge_index\"] * mask)\n    all_users = all_users_items[:len(data[\"users\"])]\n    all_items = all_users_items[len(data[\"users\"]):]\n    users_emb = all_users[users]\n    pos_emb = all_items[pos]\n    neg_emb = all_items[neg]\n    n_user = len(data[\"users\"])\n    users_emb_ego = model.embedding_user_item(users)\n    # offset the index to fetch embedding from user_item\n    pos_emb_ego = model.embedding_user_item(pos + n_user)\n    neg_emb_ego = model.embedding_user_item(neg + n_user)\n    return users_emb, pos_emb, neg_emb, users_emb_ego, pos_emb_ego, neg_emb_ego","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T20:36:09.428545Z","iopub.execute_input":"2024-10-20T20:36:09.428964Z","iopub.status.idle":"2024-10-20T20:36:09.440221Z","shell.execute_reply.started":"2024-10-20T20:36:09.428901Z","shell.execute_reply":"2024-10-20T20:36:09.439304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bpr_loss(model, users, pos, neg, data, mask):\n    \"\"\"\n    INPUT:\n        model: the LightGCN model you are training on\n        users: this is the user index (note: use 0-indexed and not user number,\n            which is 1-indexed)\n        pos: positive index corresponding to an item that the user like\n            (0-indexed, note to index items starting from 0)\n        neg: negative index corresponding to an item that the user doesn't like\n        data: the entire data, used to fetch all users and all items\n        mask: Masking matrix indicating edges present in the current\n            train / validation / test set.\n    OUTPUT:\n        loss, reg_loss\n    \"\"\"\n    # assuming we always sample the same number of positive and negative sample\n    # per user\n    assert len(users) == len(pos) and len(users) == len(neg)\n    (users_emb, pos_emb, neg_emb,\n    userEmb0,  posEmb0, negEmb0) = getEmbedding(model, users.long(), pos.long(),\n                                                neg.long(), data, mask)\n    reg_loss = (1/2)*(userEmb0.norm(2).pow(2) +\n                        posEmb0.norm(2).pow(2)  +\n                        negEmb0.norm(2).pow(2))/float(len(users))\n    pos_scores = torch.mul(users_emb, pos_emb)\n    pos_scores = torch.sum(pos_scores, dim=1)\n    neg_scores = torch.mul(users_emb, neg_emb)\n    neg_scores = torch.sum(neg_scores, dim=1)\n\n    loss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n\n    return loss, reg_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T20:36:09.441407Z","iopub.execute_input":"2024-10-20T20:36:09.441743Z","iopub.status.idle":"2024-10-20T20:36:09.453671Z","shell.execute_reply.started":"2024-10-20T20:36:09.441712Z","shell.execute_reply":"2024-10-20T20:36:09.452809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def personalized_topk(pred, K, user_indices, edge_index):\n    \"\"\"Computes TopK precision and recall.\n\n    Args:\n        pred: Predicted similarities between user and item.\n        K: Number of items to rank.\n        user_indices: Indices of users for each prediction in `pred`.\n        edge_index: User and item connection matrix.\n\n    Returns:\n        Average Top K precision and recall for users in `user_indices`.\n    \"\"\"\n    per_user_preds = collections.defaultdict(list)\n    for index, user in enumerate(user_indices):\n        per_user_preds[user.item()].append(pred[index].item())\n    precisions = 0.0\n    recalls = 0.0\n    for user, preds in per_user_preds.items():\n        while len(preds) < K:\n            preds.append(random.choice(range(edge_index.shape[1])))\n        top_ratings, top_items = torch.topk(torch.tensor(preds), K)\n        correct_preds = edge_index[user, top_items].sum().item()\n        total_pos = edge_index[user].sum().item()\n        precisions += correct_preds / K\n        recalls += correct_preds / total_pos if total_pos != 0 else 0\n    num_users = len(user_indices.unique())\n    return precisions / num_users, recalls / num_users","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T20:36:09.454826Z","iopub.execute_input":"2024-10-20T20:36:09.455232Z","iopub.status.idle":"2024-10-20T20:36:09.466162Z","shell.execute_reply.started":"2024-10-20T20:36:09.455202Z","shell.execute_reply":"2024-10-20T20:36:09.465340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _sample_pos_neg(data, mask, num_samples_per_user):\n    \"\"\"Samples (user, positive item, negative item) tuples per user.\n\n    If a user does not have a postive (negative) item, we choose an item\n    with unknown liking (an item without raw rating data).\n\n    Args:\n        data: Dataset object containing edge_index and raw ratings matrix.\n        mask: Masking matrix indicating edges present in the current\n            train / validation / test set.\n        num_samples_per_user: Number of samples to generate for each user.\n\n    Returns:\n        torch.Tensor object of (user, positive item, negative item) samples.\n    \"\"\"\n    print(\"=====Starting to sample=====\")\n    start = time.time()\n    samples = []\n    all_items = set(range(len(data[\"items\"])))\n    for user_index, user in enumerate(data[\"users\"]):\n        pos_items = set(\n            torch.nonzero(data[\"edge_index\"][user_index])[:, 0].tolist())\n        unknown_items = all_items.difference(\n                set(\n                    torch.nonzero(\n                        data[\"raw_edge_index\"][user_index])[:, 0].tolist()))\n        neg_items = all_items.difference(\n            set(pos_items)).difference(set(unknown_items))\n        unmasked_items = set(torch.nonzero(mask[user_index])[:, 0].tolist())\n        if len(unknown_items.union(pos_items)) == 0 or \\\n                len(unknown_items.union(neg_items)) == 0:\n            continue\n        for _ in range(num_samples_per_user):\n            if len(pos_items.intersection(unmasked_items)) == 0:\n                pos_item_index = random.choice(\n                    list(unknown_items.intersection(unmasked_items)))\n            else:\n                pos_item_index = random.choice(\n                    list(pos_items.intersection(unmasked_items)))\n            if len(neg_items.intersection(unmasked_items)) == 0:\n                neg_item_index = random.choice(\n                    list(unknown_items.intersection(unmasked_items)))\n            else:\n                neg_item_index = random.choice(\n                    list(neg_items.intersection(unmasked_items)))\n            samples.append((user_index, pos_item_index, neg_item_index))\n    end = time.time()\n    print(f\"=====Sampling completed (took {end - start} seconds)=====\")\n    return torch.tensor(samples, dtype=torch.int32)\n\ndef sample_pos_neg(data, train_mask, val_mask, test_mask, num_samples_per_user):\n    \"\"\"Samples (user, positive item, negative item) tuples per user.\n\n    If a user does not have a postive (negative) item, we choose an item\n    with unknown liking (an item without raw rating data).\n\n    Args:\n        data: Dataset object containing edge_index and raw ratings matrix.\n        train_mask: Masking matrix indicating edges present in train set.\n        val_mask: Masking matrix indicating edges present in validation set.\n        test_mask: Masking matrix indicating edges present in test set.\n        num_samples_per_user: Number of samples to generate for each user.\n\n    Returns:\n        torch.Tensor object of (user, positive item, negative item) samples for\n        train, validation and test.\n    \"\"\"\n    train_samples = _sample_pos_neg(data, train_mask, num_samples_per_user)\n    val_samples = _sample_pos_neg(data, val_mask, num_samples_per_user)\n    test_samples = _sample_pos_neg(data, test_mask, num_samples_per_user)\n    return train_samples, val_samples, test_samples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T20:36:09.467255Z","iopub.execute_input":"2024-10-20T20:36:09.467506Z","iopub.status.idle":"2024-10-20T20:36:09.481325Z","shell.execute_reply.started":"2024-10-20T20:36:09.467477Z","shell.execute_reply":"2024-10-20T20:36:09.480475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root = os.getcwd()\nmovielens = MovieLens(root=root, transform=trans_ml)\ndata = movielens.get()\ntrain_mask, val_mask, test_mask = \\\n        movielens.train_val_test_split(val_frac=config_dict[\"val_frac\"],\n                                       test_frac=config_dict[\"test_frac\"])\n\nn_users = len(data[\"users\"].unique())\nm_items = len(data[\"items\"].unique())\nprint(f\"#Users: {n_users}\")\nprint(f\"#Items: {m_items}\")\n\nmodel_config = {\n    \"n_users\": n_users,\n    \"m_items\": m_items,\n    \"embedding_size\": config_dict[\"embedding_size\"],\n    \"num_layers\": config_dict[\"num_layers\"],\n}\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlightGCN = LightGCN(model_config, device=device)\n\nnum_samples_per_user = config_dict[\"num_samples_per_user\"]\nepochs = config_dict[\"epochs\"]\nbatch_size = config_dict[\"batch_size\"]\nlr = config_dict[\"lr\"]\nweight_decay = config_dict[\"weight_decay\"]\n\nK = config_dict[\"K\"]\n\nlightGCN.to(device)\n\nsamples_train, samples_val, samples_test = \\\n        sample_pos_neg(data, train_mask, val_mask, test_mask,\n                       num_samples_per_user)\n\nsamples_train=samples_train.to(device)\nsamples_val=samples_val.to(device)\nsamples_test=samples_test.to(device)\ntrain_mask=train_mask.to(device)\nval_mask=val_mask.to(device)\ntest_mask=test_mask.to(device)\ndata = data.to(device)\n\nprint(f\"#Training samples: {len(samples_train)}\",\n      f\"#Validation samples: {len(samples_val)}\",\n      f\"#Test samples: {len(samples_test)}\")\n\noptimizer = optim.Adam(lightGCN.parameters(), lr=lr)\nprint(\"Optimizer:\", optimizer)\n\nepochs_tracked = []\ntrain_topks = []\nval_topks = []\n\nfor epoch in range(epochs):\n    print(\"Training on the {} epoch\".format(epoch))\n    lightGCN.train()\n    loss_sum = 0\n    # Shuffle the order of rows.\n    samples_train = samples_train[torch.randperm(samples_train.size()[0])]\n    for batch_idx in range(math.ceil(len(samples_train) / batch_size)):\n        optimizer.zero_grad()\n\n        current_batch = \\\n            samples_train[batch_idx*batch_size: (batch_idx+1)*batch_size]\n        # Shuffle the order of rows.\n        current_batch = current_batch[torch.randperm(current_batch.size()[0])]\n        users = current_batch[:, 0:1]\n        pos = current_batch[:, 1:2]\n        neg = current_batch[:, 2:3]\n\n        loss, reg_loss = bpr_loss(lightGCN, users, pos, neg, data,\n                                  train_mask)\n        reg_loss = reg_loss * weight_decay\n        loss = loss + reg_loss\n        loss_sum += loss.detach()\n\n        loss.backward()\n        optimizer.step()\n\n        if batch_idx % config_dict[\"minibatch_per_print\"] == 0:\n            all_users = torch.linspace(start=0,\n                                       end=n_users - 1, steps=n_users).long()\n            user_indices = current_batch[:, 0]\n            user_indices = user_indices.repeat(2).long()\n            item_indices = torch.cat(\n                (current_batch[:, 1], current_batch[:, 2])).long()\n            pred = getUsersRating(lightGCN,\n                                  all_users,\n                                  data)[user_indices, item_indices]\n            truth = data[\"edge_index\"][user_indices, item_indices]\n            topk_precision, topk_recall = \\\n                personalized_topk(pred, K, user_indices, data[\"edge_index\"])\n\n            print(\"Training on epoch {} minibatch {}/{} completed\\n\".format(epoch, batch_idx+1,\n                                                                            math.ceil(len(samples_train) / batch_size)),\n                  \"bpr_loss on current minibatch is {}, and regularization loss is {}.\\n\".format(round(float(loss.detach().cpu()), 6),\n                                                                                                 round(float(reg_loss.detach().cpu()), 6)),\n                  \"Top K precision = {}, recall = {}.\".format(topk_precision, topk_recall))\n\n    if epoch % config_dict[\"epochs_per_print\"] == 0:\n        epochs_tracked.append(epoch)\n\n        # evaluation on both the trainisng and validation set\n        lightGCN.eval()\n        # predict on the training set\n        users = samples_train[:, 0:1]\n        user_indices = samples_train[:, 0]\n        user_indices = user_indices.repeat(2).long()\n        item_indices = torch.cat(\n            (samples_train[:, 1], samples_train[:, 2])).long()\n        pred = getUsersRating(lightGCN,\n                              users[:,0],\n                              data)[user_indices, item_indices]\n        truth = data[\"edge_index\"][users.long()[:,0]]\\\n            [user_indices, item_indices]\n        train_topk_precision, train_topk_recall = \\\n            personalized_topk(pred, K, user_indices, data[\"edge_index\"])\n        train_topks.append((train_topk_precision, train_topk_recall))\n\n        # predict on the validation set\n        users_val = samples_val[:, 0:1]\n        pos_val = samples_val[:, 1:2]\n        neg_val = samples_val[:, 2:3]\n\n        loss_val, reg_loss_val = bpr_loss(\n            lightGCN, users_val, pos_val, neg_val, data, val_mask)\n        reg_loss_val = reg_loss_val * weight_decay\n\n        # predict on the validation set\n        user_indices = samples_val[:, 0]\n        user_indices = user_indices.repeat(2).long()\n        item_indices = torch.cat((samples_val[:, 1], samples_val[:, 2])).long()\n        pred_val = getUsersRating(lightGCN,\n                                  users_val[:,0],\n                                  data)[user_indices, item_indices]\n        truth_val = data[\"edge_index\"][users_val.long()[:,0]]\\\n            [user_indices, item_indices]\n        val_topk_precision, val_topk_recall = \\\n            personalized_topk(pred_val, K, user_indices, data[\"edge_index\"])\n        val_topks.append((val_topk_precision, val_topk_recall))\n\n        print(\"\\nTraining on {} epoch completed.\\n\".format(epoch),\n              \"Average bpr_loss on train set is {} for the current epoch.\\n\".format(round(float(loss_sum/len(samples_train)), 6)),\n              \"Training top K precision = {}, recall = {}.\\n\".format(train_topk_precision, train_topk_recall),\n              \"Average bpr_loss on the validation set is {}, and regularization loss is {}.\\n\".format(round(float((loss_val+reg_loss_val)/len(samples_val)), 6),\n                                                                                                      round(float(reg_loss_val/len(samples_val)), 6)),\n              \"Validation top K precision = {}, recall = {}.\\n\".format(val_topk_precision, val_topk_recall))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T20:36:09.482438Z","iopub.execute_input":"2024-10-20T20:36:09.482709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(epochs_tracked, [precision for precision, _ in train_topks],\n         label=\"Train\")\nplt.plot(epochs_tracked, [precision for precision, _ in val_topks],\n         label=\"Val\")\nplt.ylabel(f\"Top {K} precision\")\nplt.xlabel(\"Epochs\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(epochs_tracked, [recal for _, recal in train_topks],\n         label=\"Train\")\nplt.plot(epochs_tracked, [recal for _, recal in val_topks],\n         label=\"Val\")\nplt.ylabel(f\"Top {K} recal\")\nplt.xlabel(\"Epochs\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"figure, axis = plt.subplots(2)\naxis[0].plot(epochs_tracked, [precision for precision, _ in train_topks],\n         label=\"Train\")\naxis[0].plot(epochs_tracked, [precision for precision, _ in val_topks],\n         label=\"Val\")\n#axis[0].ylabel(f\"Top {K} precision\")\n#axis[0].xlabel(\"Epochs\")\n\naxis[1].plot(epochs_tracked, [recal for precision, recal in train_topks],\n         label=\"Train\")\naxis[ 1].plot(epochs_tracked, [recal for precision, recal in val_topks],\n         label=\"Val\")\naxis[ 1].ylabel(f\"Top {K} recal\")\naxis[ 1].xlabel(\"Epochs\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# predict on the test set\nlightGCN.eval()\nprint(\"Training completed after {} epochs\".format(epochs))\n\nusers_test = samples_test[:, 0:1]\npos_test = samples_test[:, 1:2]\nneg_test = samples_test[:, 2:3]\n\nloss_test, reg_loss_test = bpr_loss(\n    lightGCN, users_test, pos_test, neg_test, data, test_mask)\nreg_loss_test = reg_loss_test * weight_decay\n\n# predict on the test set\nuser_indices = samples_test[:, 0]\nuser_indices = user_indices.repeat(2).long()\nitem_indices = torch.cat((samples_test[:, 1], samples_test[:, 2])).long()\npred_test = getUsersRating(lightGCN, users_test[:,0], data)\\\n    [user_indices, item_indices]\ntruth_test = data[\"edge_index\"][users_test.long()[:,0]]\\\n    [user_indices, item_indices]\ntest_topk_precision, test_topk_recall = personalized_topk(\n    pred_test, K, user_indices, data[\"edge_index\"])\n\nprint(\"Average bpr_loss on the test set is {}, and regularization loss is {}.\\n\".format(round(float((loss_test+reg_loss_test)/len(samples_test)), 6),\n                                                                                                round(float(reg_loss_test/len(samples_test)), 6)),\n      \"Top K precision = {}, recall = {}.\".format(test_topk_precision, test_topk_recall))\n\n# Save model embeddings.\ntorch.save(lightGCN, config_dict[\"model_name\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}