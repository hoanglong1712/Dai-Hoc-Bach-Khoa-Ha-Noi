{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOYeiL6FPCRj8zkXZzw5y99"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5V8GVdMEjN3m"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"kPt5q27L5557"},"source":["# CS246 - Colab 1\n","## Word Count in Spark"]},{"cell_type":"markdown","metadata":{"id":"p0-YhEpP_Ds-"},"source":["### Setup"]},{"cell_type":"markdown","metadata":{"id":"Zsj5WYpR9QId"},"source":["Let's set up Spark on your Colab environment.  Run the cell below!"]},{"cell_type":"code","metadata":{"id":"k-qHai2252mI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731301381180,"user_tz":300,"elapsed":29336,"user":{"displayName":"tong cuc thue lam viec tai","userId":"05791717440652007057"}},"outputId":"1fb687ab-8883-48b1-f82e-deb8061f6a96"},"source":["!pip install pyspark\n","!pip install -U -q PyDrive2\n","#the output 'xxx is not a symbolic link' will not affect your implementation or execution\n","#to fix 'xxx is not a symbolic link', you can comment out the lines starting from !mv xxxx\n","#you may need to replace xxx.11 with the correct version if other errors come up after colab update\n","#to get the correct version, use !ls /usr/local/lib to find out\n","!mv /usr/local/lib/libtbbmalloc_proxy.so.2 /usr/local/lib/libtbbmalloc_proxy.so.2.backup\n","!mv /usr/local/lib/libtbbmalloc.so.2 /usr/local/lib/libtbbmalloc.so.2.backup\n","!mv /usr/local/lib/libtbbbind_2_5.so.3 /usr/local/lib/libtbbbind_2_5.so.3.backup\n","!mv /usr/local/lib/libtbb.so.12 /usr/local/lib/libtbb.so.12.backup\n","!mv /usr/local/lib/libtbbbind_2_0.so.3 /usr/local/lib/libtbbbind_2_0.so.3.backup\n","!mv /usr/local/lib/libtbbbind.so.3 /usr/local/lib/libtbbbind.so.3.backup\n","!ln -s /usr/local/lib/libtbbmalloc_proxy.so.2.11 /usr/local/lib/libtbbmalloc_proxy.so.2\n","!ln -s /usr/local/lib/libtbbmalloc.so.2.11 /usr/local/lib/libtbbmalloc.so.2\n","!ln -s /usr/local/lib/libtbbbind_2_5.so.3.11 /usr/local/lib/libtbbbind_2_5.so.3\n","!ln -s /usr/local/lib/libtbb.so.12.11 /usr/local/lib/libtbb.so.12\n","!ln -s /usr/local/lib/libtbbbind_2_0.so.3.11 /usr/local/lib/libtbbbind_2_0.so.3\n","!ln -s /usr/local/lib/libtbbbind.so.3.11 /usr/local/lib/libtbbbind.so.3\n","# !sudo ldconfig\n","#If error related to the above execution occurs, you can try commenting out the above 12 lines under pip install PyDrive2 (not included)\n","!apt install openjdk-8-jdk-headless -qq\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","#the output 'xxx is not a symbolic link' will not affect your implementation or execution\n","#to fix 'xxx is not a symbolic link', you can comment out the lines starting from !mv xxxx\n","#you may need to replace xxx.11 with the correct version if other errors come up after colab update\n","#to get the correct version, use !ls /usr/local/lib to find out\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hThe following additional packages will be installed:\n","  libxtst6 openjdk-8-jre-headless\n","Suggested packages:\n","  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n","  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n","The following NEW packages will be installed:\n","  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n","0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n","Need to get 39.6 MB of archives.\n","After this operation, 144 MB of additional disk space will be used.\n","Selecting previously unselected package libxtst6:amd64.\n","(Reading database ... 123623 files and directories currently installed.)\n","Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n","Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n","Selecting previously unselected package openjdk-8-jre-headless:amd64.\n","Preparing to unpack .../openjdk-8-jre-headless_8u422-b05-1~22.04_amd64.deb ...\n","Unpacking openjdk-8-jre-headless:amd64 (8u422-b05-1~22.04) ...\n","Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n","Preparing to unpack .../openjdk-8-jdk-headless_8u422-b05-1~22.04_amd64.deb ...\n","Unpacking openjdk-8-jdk-headless:amd64 (8u422-b05-1~22.04) ...\n","Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n","Setting up openjdk-8-jre-headless:amd64 (8u422-b05-1~22.04) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n","Setting up openjdk-8-jdk-headless:amd64 (8u422-b05-1~22.04) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"-CJ71AKe91eh"},"source":["Now we authenticate a Google Drive client to download the file we will be processing in our Spark job.\n","\n","**Make sure to follow the interactive instructions.**"]},{"cell_type":"code","metadata":{"id":"5K93ABEy9Zlo"},"source":["from pydrive2.auth import GoogleAuth\n","from pydrive2.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0orRvrc1-545"},"source":["id='1SE6k_0YukzGd5wK-E4i6mG83nydlfvSa'\n","downloaded = drive.CreateFile({'id': id})\n","downloaded.GetContentFile('pg100.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qwtlO4_m_LbQ"},"source":["If you executed the cells above, you should be able to see the file *pg100.txt* under the \"Files\" tab on the left panel."]},{"cell_type":"markdown","metadata":{"id":"CRaF2A_j_nC7"},"source":["### Your task"]},{"cell_type":"markdown","metadata":{"id":"ebLNUxP0_8x3"},"source":["If you successfully run the setup stage, you are ready to work on the *pg100.txt* file which contains a copy of the complete works of Shakespeare.\n","\n","Write a Spark application which outputs the number of words that start with each letter. This means that for every letter, we want to count the total number of (non-unique) words that start with a specific letter. (If a specific (aka unique) word that starts with letter 'a' appears N times, it should be counted in words starting with 'a' N times.)\n","\n","In your implementation, **ignore the letter case**, i.e., consider all words as lower case. Also, you can ignore all words that **start** with a non-alphabetic character. You should output word counts for the **entire document**, inclusive of the title, author, and the main texts. If you encounter words broken as a result of new lines, e.g. \"pro-ject\" where the segment after the dash sign is on a new line, no special processing is needed and you can safely consider it as two words (\"pro\" and \"ject\").\n","\n","Your outputs will be graded on a range -- if your differences from the ground-truths are within an error threshold of 5, you'll be considered correct.\n","\n","**Hint:**\n","1. split only on space (' ') but not hyphen/dash ('-') or other symbols.\n","2. you may find spark functions explode (https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.explode.html) and split (https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.split.html) helpful, but you don't need to restrict to them as long as you can satisfy your goal.\n","\n","Clarification:\n","\n","1. If a word 'project' is separated into two lines in the form of 'pro-' in the first line and 'ject' in the second line, it should be treated as two words (when you import the text using spark.read.text, it treats each newline as a new row in the DataFrame). However, for the word 'self-love' that appears in a single line, it should be treated as one word starting with letter 's'.\n","\n"]},{"cell_type":"code","metadata":{"id":"xu-e7Ph2_ruG"},"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext\n","import pandas as pd\n","\n","# create the Spark Session\n","spark = SparkSession.builder.getOrCreate()\n","\n","# create the Spark Context\n","sc = spark.sparkContext"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AuAxGFPFB43Y"},"source":["# YOUR\n","txt = spark.read.text(\"pg100.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = [\"Project Gutenberg’s\",\n","        \"Alice’s Adventures in Wonderland\",\n","        \"Project Gutenberg’s\",\n","        \"Adventures in Wonderland\",\n","        \"Project Gutenberg’s\"]\n","rdd=spark.sparkContext.parallelize(data)\n","#for element in rdd.collect():\n","#    print(element)\n","rdd2 = rdd.flatMap(lambda x: x.split(\" \"))\n","for element in rdd2.collect():\n","    print(element)\n","\n","rdd2.map(lambda x: (x[0].lower(), 1)).reduceByKey(lambda a, b: a + b).collect()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zji5dJROrGon","executionInfo":{"status":"ok","timestamp":1731302661767,"user_tz":300,"elapsed":1566,"user":{"displayName":"tong cuc thue lam viec tai","userId":"05791717440652007057"}},"outputId":"ea65f05e-864a-4b9c-f877-0efeb342842a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Project\n","Gutenberg’s\n","Alice’s\n","Adventures\n","in\n","Wonderland\n","Project\n","Gutenberg’s\n","Adventures\n","in\n","Wonderland\n","Project\n","Gutenberg’s\n"]},{"output_type":"execute_result","data":{"text/plain":["[('p', 3), ('g', 3), ('i', 2), ('a', 3), ('w', 2)]"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# CODE\n","\n","'''\n","txt.rdd.flatMap(lambda x: x.value.split(\" \")) \\\n","  .filter(lambda x: x)\\ # remove empty strings\n","  .map(lambda x: (x[0].lower(), 1))\\ # map to (letter, 1) letter is the firct charater of word\n","  .filter(lambda x: x[0].isalpha()) \\ # remove non-alphabetic characters\n","  .reduceByKey(lambda a, b: a + b)\\ # reduce by key\n","  .sortByKey() # sort by key\n","'''\n","result = txt.rdd.flatMap(lambda x: x.value.split(\" \")) \\\n","  .filter(lambda x: x)\\\n","  .map(lambda x: (x[0].lower(), 1))\\\n","  .filter(lambda x: x[0].isalpha()) \\\n","  .reduceByKey(lambda a, b: a + b)\\\n","  .sortByKey()"],"metadata":{"id":"fU-euN6nX9-a"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7jDCs412ZZcF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731303596645,"user_tz":300,"elapsed":806,"user":{"displayName":"tong cuc thue lam viec tai","userId":"05791717440652007057"}},"outputId":"5acb2a59-066d-4303-c5b6-fbc661128454"},"source":["# HERE\n","#result.collect()\n","for letter, count in result.collect():\n","    print(f\"{letter}: {count}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a: 84836\n","b: 45455\n","c: 34567\n","d: 29713\n","e: 18697\n","f: 36814\n","g: 20782\n","h: 60563\n","i: 62167\n","j: 3339\n","k: 9418\n","l: 29569\n","m: 55676\n","n: 26759\n","o: 43494\n","p: 27759\n","q: 2377\n","r: 14265\n","s: 65705\n","t: 123602\n","u: 9170\n","v: 5728\n","w: 59597\n","x: 14\n","y: 25855\n","z: 71\n"]}]},{"cell_type":"markdown","metadata":{"id":"SIrXJyVNP2AI"},"source":["Once you obtained the desired results, **head over to Gradescope and submit your solution for this Colab**!"]}]}